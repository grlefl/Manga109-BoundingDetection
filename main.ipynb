{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Sources  \n",
    "- http://host.robots.ox.ac.uk/pascal/VOC/voc2007/ (pascal dataset)\n",
    "- https://github.com/keshik6/pascal-voc-classification/tree/master (pascal github inspo)\n",
    "- http://host.robots.ox.ac.uk/pascal/VOC/voc2007/devkit_doc_07-Jun-2007.pdf (pascal devkit)\n",
    "- https://github.com/FabrizioDeSantis/Object-Detection-Manga109/tree/main (manga109 github inspo)\n",
    "- manga109 dataset \\\n",
    "- manga109 libraries, api \n",
    "- manga109 paper \n",
    "- https://towardsdatascience.com/bounding-box-prediction-from-scratch-using-pytorch-a8525da51ddc "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e90913e53d021ca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dependencies "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f2b894c2c605ede"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-08T03:23:55.174926Z",
     "start_time": "2024-04-08T03:23:50.052821Z"
    }
   },
   "outputs": [],
   "source": [
    "import manga109api_custom\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T \n",
    "from dataset import CustomDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as A \n",
    "import pandas as pd\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load Data (fix paths)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5ff89f8469ec2f9"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                img_path  width  height  \\\n",
      "0      C:\\Users\\Grace\\Documents\\Manga109-BoundingDete...   1654    1170   \n",
      "1      C:\\Users\\Grace\\Documents\\Manga109-BoundingDete...   1654    1170   \n",
      "2      C:\\Users\\Grace\\Documents\\Manga109-BoundingDete...   1654    1170   \n",
      "3      C:\\Users\\Grace\\Documents\\Manga109-BoundingDete...   1654    1170   \n",
      "4      C:\\Users\\Grace\\Documents\\Manga109-BoundingDete...   1654    1170   \n",
      "...                                                  ...    ...     ...   \n",
      "10125  C:\\Users\\Grace\\Documents\\Manga109-BoundingDete...   1654    1170   \n",
      "10126  C:\\Users\\Grace\\Documents\\Manga109-BoundingDete...   1654    1170   \n",
      "10127  C:\\Users\\Grace\\Documents\\Manga109-BoundingDete...   1654    1170   \n",
      "10128  C:\\Users\\Grace\\Documents\\Manga109-BoundingDete...   1654    1170   \n",
      "10129  C:\\Users\\Grace\\Documents\\Manga109-BoundingDete...   1654    1170   \n",
      "\n",
      "              book_id                                             bboxes  \\\n",
      "0                ARMS  [[83, 86, 668, 1004], [406, 684, 50, 80], [178...   \n",
      "1                ARMS  [[899, 585, 271, 500], [2, 0, 824, 513], [72, ...   \n",
      "2                ARMS  [[449, 593, 290, 499], [908, 6, 668, 466], [73...   \n",
      "3                ARMS  [[86, 91, 663, 337], [915, 568, 666, 531], [82...   \n",
      "4                ARMS  [[2, 672, 817, 425], [902, 94, 379, 218], [832...   \n",
      "...               ...                                                ...   \n",
      "10125  YumeiroCooking  [[597, 0, 149, 508], [910, 609, 221, 560], [91...   \n",
      "10126  YumeiroCooking  [[404, 258, 341, 468], [907, 520, 660, 188], [...   \n",
      "10127  YumeiroCooking  [[91, 70, 384, 237], [908, 317, 403, 156], [48...   \n",
      "10128  YumeiroCooking  [[1403, 480, 250, 647], [911, 70, 312, 262], [...   \n",
      "10129  YumeiroCooking  [[1345, 73, 225, 476], [1201, 76, 136, 286], [...   \n",
      "\n",
      "                                                  labels  \n",
      "0                                           [4, 1, 2, 3]  \n",
      "1      [4, 4, 4, 4, 4, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, ...  \n",
      "2      [4, 4, 4, 4, 4, 4, 4, 1, 1, 1, 1, 1, 1, 2, 2, ...  \n",
      "3      [4, 4, 4, 4, 4, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, ...  \n",
      "4      [4, 4, 4, 4, 4, 4, 4, 4, 1, 1, 1, 1, 1, 2, 2, ...  \n",
      "...                                                  ...  \n",
      "10125  [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 1, 1, 1, ...  \n",
      "10126  [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 1, ...  \n",
      "10127  [4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 1, 1, 1, 1, 1, ...  \n",
      "10128  [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, ...  \n",
      "10129  [4, 4, 4, 4, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
      "\n",
      "[10130 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# directory with Manga109 data \n",
    "manga109_root_dir = \"./Manga109/Manga109_released_2023_12_07\" # directory with Manga109 data\n",
    "\n",
    "# custom parser to access Manga109 data \n",
    "p = manga109api_custom.Parser(root_dir=manga109_root_dir)\n",
    "img_dict = p.load_all_images()\n",
    "\n",
    "# create pandas dataframe from the dictionary\n",
    "df = pd.DataFrame(img_dict)\n",
    "print(df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T03:24:04.288050Z",
     "start_time": "2024-04-08T03:23:55.175984Z"
    }
   },
   "id": "780b16e02f56126d",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "Split and Stratify Data "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb8ac8c465044628"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Images: 10130\n",
      "Training Images: 6483\n",
      "Validation Images: 1621\n",
      "Testing Images: 2026\n"
     ]
    }
   ],
   "source": [
    "test_ratio = 0.2\n",
    "val_ratio = 0.2\n",
    "\n",
    "# split data into training/validation and testing sets\n",
    "train_val_df, test_df = train_test_split(df, test_size=test_ratio, stratify=df['book_id'], random_state=42)\n",
    "\n",
    "# further split into training and validation sets \n",
    "train_df, val_df = train_test_split(train_val_df, test_size=val_ratio, stratify=train_val_df['book_id'], random_state=42)\n",
    "\n",
    "print(f\"Total Images: {len(df)}\")\n",
    "print(f\"Training Images: {len(train_df)}\")\n",
    "print(f\"Validation Images: {len(val_df)}\")\n",
    "print(f\"Testing Images: {len(test_df)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T03:24:04.316288Z",
     "start_time": "2024-04-08T03:24:04.289118Z"
    }
   },
   "id": "e176591c33fabaea",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "Transforms"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b364729dd91ae76"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# define training transform\n",
    "train_transform = A.Compose([ \n",
    "    A.CenterCrop(height=1170, width=1650, p=1),                                 # crop first                         \n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    #A.ColorJitter(brightness=(0.80, 1.20)),\n",
    "    #A.Rotate(limit=90,p=0.5),                 \n",
    "    #A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    A.Resize(height=780, width=1100, interpolation=1, p=1),                    # resize last \n",
    "    ToTensorV2()                                                               # to tensor\n",
    "], bbox_params=A.BboxParams(format='coco',label_fields=['labels'])) \n",
    "\n",
    "# define validation transform\n",
    "val_transform = A.Compose([\n",
    "    A.CenterCrop(height=1170, width=1650, p=1),\n",
    "    A.Resize(height=780, width=1100, interpolation=1, p=1),\n",
    "    ToTensorV2()\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T03:24:04.324047Z",
     "start_time": "2024-04-08T03:24:04.318374Z"
    }
   },
   "id": "c335555ad0621fdb",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create Datasets & DataLoaders "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c33c7bd173bdece"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# datasets\n",
    "train_dataset = CustomDataset(train_df, train_transform)\n",
    "val_dataset = CustomDataset(val_df, val_transform)\n",
    "\n",
    "# dataloaders \n",
    "train_loader = DataLoader(train_dataset, batch_size=8, num_workers=4, shuffle=True)\n",
    "valid_loader = DataLoader(val_dataset, batch_size=8, num_workers=4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T03:24:04.329621Z",
     "start_time": "2024-04-08T03:24:04.325088Z"
    }
   },
   "id": "abcb7430c9aaa0d7",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualize Data - testing & validation images  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a0a43f86d17652"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\Grace\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Grace\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Grace\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 277, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Grace\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 144, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Grace\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 121, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Grace\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 174, in collate_tensor_fn\n    return torch.stack(batch, 0, out=out)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: stack expects each tensor to be equal size, but got [22, 4] at entry 0 and [121, 4] at entry 1\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m image \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43miter\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m fig, ax \u001B[38;5;241m=\u001B[39m plt\u001B[38;5;241m.\u001B[39msubplots(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m, figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m6\u001B[39m, \u001B[38;5;241m4\u001B[39m))\n\u001B[0;32m      4\u001B[0m ax\u001B[38;5;241m.\u001B[39mimshow(np\u001B[38;5;241m.\u001B[39mtranspose(np\u001B[38;5;241m.\u001B[39masarray(image[\u001B[38;5;241m0\u001B[39m]), (\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m0\u001B[39m)))\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    630\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 631\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    635\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1346\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1344\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1345\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_task_info[idx]\n\u001B[1;32m-> 1346\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_process_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1372\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._process_data\u001B[1;34m(self, data)\u001B[0m\n\u001B[0;32m   1370\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_try_put_index()\n\u001B[0;32m   1371\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, ExceptionWrapper):\n\u001B[1;32m-> 1372\u001B[0m     \u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreraise\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1373\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_utils.py:722\u001B[0m, in \u001B[0;36mExceptionWrapper.reraise\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    718\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m    719\u001B[0m     \u001B[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001B[39;00m\n\u001B[0;32m    720\u001B[0m     \u001B[38;5;66;03m# instantiate since we don't know how to\u001B[39;00m\n\u001B[0;32m    721\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 722\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exception\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\Grace\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Grace\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Grace\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 277, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Grace\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 144, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Grace\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 121, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Grace\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 174, in collate_tensor_fn\n    return torch.stack(batch, 0, out=out)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: stack expects each tensor to be equal size, but got [22, 4] at entry 0 and [121, 4] at entry 1\n"
     ]
    }
   ],
   "source": [
    "image = next(iter(train_loader))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "ax.imshow(np.transpose(np.asarray(image[0]), (1, 2, 0)))\n",
    "ax.set_title('Please Draw Boxes')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T03:24:16.047350Z",
     "start_time": "2024-04-08T03:24:04.330712Z"
    }
   },
   "id": "a2176c866932c4f0",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(type(bboxes))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T03:24:16.049550Z",
     "start_time": "2024-04-08T03:24:16.048493Z"
    }
   },
   "id": "30c564a1d754155a",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define and Train Model "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c56b47c0ea845f9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = models.resnet34\n",
    "# optimizer = something \n",
    "# loss function = something \n",
    "# epochs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-08T03:24:16.051714Z"
    }
   },
   "id": "551aaea05c6e2845"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Validation Metrics \n",
    "- Testing Images Visualization \n",
    "- Testing \n",
    "- Testing Evaluation Metrics "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f76ffc506bbd1347"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
