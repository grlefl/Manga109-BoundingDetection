{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Sources  \n",
    "- http://host.robots.ox.ac.uk/pascal/VOC/voc2007/ (pascal dataset)\n",
    "- https://github.com/keshik6/pascal-voc-classification/tree/master (pascal github inspo)\n",
    "- http://host.robots.ox.ac.uk/pascal/VOC/voc2007/devkit_doc_07-Jun-2007.pdf (pascal devkit)\n",
    "- https://github.com/FabrizioDeSantis/Object-Detection-Manga109/tree/main (manga109 github inspo)\n",
    "- manga109 dataset \\\n",
    "- manga109 libraries, api \n",
    "- manga109 paper \n",
    "- https://towardsdatascience.com/bounding-box-prediction-from-scratch-using-pytorch-a8525da51ddc "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e90913e53d021ca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dependencies "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f2b894c2c605ede"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-07T18:39:20.743201Z",
     "start_time": "2024-04-07T18:39:15.470603Z"
    }
   },
   "outputs": [],
   "source": [
    "import manga109api_custom\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T \n",
    "import torch \n",
    "from dataset import CustomDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as A "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load and Split Data "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5ff89f8469ec2f9"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images: 10130\n",
      "Number of training images: 6483\n",
      "Number of validation images: 1621\n",
      "Number of testing images: 2026\n"
     ]
    }
   ],
   "source": [
    "# directory with Manga109 data \n",
    "manga109_root_dir = \"./Manga109/Manga109_released_2023_12_07\" # directory with Manga109 data\n",
    "\n",
    "# custom parser to access Manga109 data \n",
    "p = manga109api_custom.Parser(root_dir=manga109_root_dir)\n",
    "\n",
    "# images = [{'img_path','page_annotation'},...] # list of image dictionaries  \n",
    "# author_labels = ['book1','book2','book1',...] # corresponding book label associated with each image\n",
    "images, book_labels = p.load_all_images()\n",
    "\n",
    "# define the proportions for initial split (e.g., 80% train/validation, 20% test) #CHECK \n",
    "train_val_ratio = 0.8\n",
    "test_ratio = 0.2\n",
    "\n",
    "# split data into training/validation and testing sets\n",
    "train_val_images, test_images, train_val_labels, _ = train_test_split(\n",
    "    images, book_labels, stratify=book_labels, test_size=test_ratio, random_state=42\n",
    ")\n",
    "\n",
    "# further split the training/validation set into training and validation sets\n",
    "train_images, val_images, _, _ = train_test_split(\n",
    "    train_val_images, train_val_labels, stratify=train_val_labels, test_size=1 - train_val_ratio, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Total number of images: {len(images)}\")\n",
    "print(f\"Number of training images: {len(train_images)}\")\n",
    "print(f\"Number of validation images: {len(val_images)}\")\n",
    "print(f\"Number of testing images: {len(test_images)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T18:39:29.975607Z",
     "start_time": "2024-04-07T18:39:20.745248Z"
    }
   },
   "id": "e176591c33fabaea",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "Transformations"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b364729dd91ae76"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# define training transformations\n",
    "train_transformations = A.Compose([\n",
    "    # resize? \n",
    "    #T.PILToTensor(),                            \n",
    "    #T.ConvertImageDtype(torch.float),           \n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    T.ColorJitter(brightness=(0.80, 1.20)),\n",
    "    A.Rotate(limit=90,p=0.5),\n",
    "    A.RandomCropFromBorders(crop_left=0.1, crop_right=0.1, crop_top=0.1, crop_bottom=0.1, always_apply=False, p=1.0)\n",
    "], bbox_params=A.BboxParams(format='coco'))\n",
    "\n",
    "# define validation transformations\n",
    "val_transformations = T.Compose([\n",
    "    # resize?\n",
    "    #T.PILToTensor(),\n",
    "    #T.ConvertImageDtype(torch.float)\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T18:39:29.984092Z",
     "start_time": "2024-04-07T18:39:29.976610Z"
    }
   },
   "id": "c335555ad0621fdb",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create Datasets & DataLoaders "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c33c7bd173bdece"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_images,train_transformations)\n",
    "val_dataset = CustomDataset(val_images)\n",
    "\n",
    "batch_size = 8\n",
    "num_workers = 4\n",
    "\n",
    "# image, target \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "valid_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T18:39:29.991151Z",
     "start_time": "2024-04-07T18:39:29.985094Z"
    }
   },
   "id": "abcb7430c9aaa0d7",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualize Data - testing & validation images  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a0a43f86d17652"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\Grace\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Grace\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"C:\\Users\\Grace\\Documents\\Manga109-BoundingDetection\\dataset.py\", line 59, in __getitem__\n    image, my_annotation = self.transform(image=image_np, bboxes=bboxes)\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Grace\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\albumentations\\core\\composition.py\", line 211, in __call__\n    p.ensure_data_valid(data)\n  File \"C:\\Users\\Grace\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\albumentations\\core\\bbox_utils.py\", line 116, in ensure_data_valid\n    raise ValueError(msg)\nValueError: Please specify 'label_fields' in 'bbox_params' or add labels to the end of bbox because bboxes must have labels\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdataset\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m visualize_image_with_boxes\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtransforms\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctional\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mF\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m image, annotation \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43miter\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    630\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 631\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    635\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1346\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1344\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1345\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_task_info[idx]\n\u001B[1;32m-> 1346\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_process_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1372\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._process_data\u001B[1;34m(self, data)\u001B[0m\n\u001B[0;32m   1370\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_try_put_index()\n\u001B[0;32m   1371\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, ExceptionWrapper):\n\u001B[1;32m-> 1372\u001B[0m     \u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreraise\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1373\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_utils.py:722\u001B[0m, in \u001B[0;36mExceptionWrapper.reraise\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    718\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m    719\u001B[0m     \u001B[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001B[39;00m\n\u001B[0;32m    720\u001B[0m     \u001B[38;5;66;03m# instantiate since we don't know how to\u001B[39;00m\n\u001B[0;32m    721\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 722\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exception\n",
      "\u001B[1;31mValueError\u001B[0m: Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\Grace\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Grace\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"C:\\Users\\Grace\\Documents\\Manga109-BoundingDetection\\dataset.py\", line 59, in __getitem__\n    image, my_annotation = self.transform(image=image_np, bboxes=bboxes)\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Grace\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\albumentations\\core\\composition.py\", line 211, in __call__\n    p.ensure_data_valid(data)\n  File \"C:\\Users\\Grace\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\albumentations\\core\\bbox_utils.py\", line 116, in ensure_data_valid\n    raise ValueError(msg)\nValueError: Please specify 'label_fields' in 'bbox_params' or add labels to the end of bbox because bboxes must have labels\n"
     ]
    }
   ],
   "source": [
    "from dataset import visualize_image_with_boxes\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "image, annotation = next(iter(train_loader))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T18:39:44.136593Z",
     "start_time": "2024-04-07T18:39:29.993156Z"
    }
   },
   "id": "a2176c866932c4f0",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define and Train Model "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c56b47c0ea845f9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = models.resnet34\n",
    "# optimizer = something \n",
    "# loss function = something \n",
    "# epochs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-07T18:39:44.137594Z"
    }
   },
   "id": "551aaea05c6e2845"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Validation Metrics \n",
    "- Testing Images Visualization \n",
    "- Testing \n",
    "- Testing Evaluation Metrics "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f76ffc506bbd1347"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
