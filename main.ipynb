{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Sources  \n",
    "- http://host.robots.ox.ac.uk/pascal/VOC/voc2007/ (pascal dataset)\n",
    "- https://github.com/keshik6/pascal-voc-classification/tree/master (pascal github inspo)\n",
    "- http://host.robots.ox.ac.uk/pascal/VOC/voc2007/devkit_doc_07-Jun-2007.pdf (pascal devkit)\n",
    "- https://github.com/FabrizioDeSantis/Object-Detection-Manga109/tree/main (manga109 github inspo)\n",
    "- manga109 dataset \\\n",
    "- manga109 libraries, api \n",
    "- manga109 paper \n",
    "- https://towardsdatascience.com/bounding-box-prediction-from-scratch-using-pytorch-a8525da51ddc "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e90913e53d021ca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dependencies "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f2b894c2c605ede"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-07T21:07:46.539667Z",
     "start_time": "2024-04-07T21:07:40.229923Z"
    }
   },
   "outputs": [],
   "source": [
    "import manga109api_custom\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T \n",
    "import torch \n",
    "from dataset import CustomDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as A \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load Data (fix paths)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5ff89f8469ec2f9"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                img_path  width  height  \\\n",
      "0      C:\\Users\\Grace\\Documents\\Manga109-BoundingDete...   1654    1170   \n",
      "1      C:\\Users\\Grace\\Documents\\Manga109-BoundingDete...   1654    1170   \n",
      "2      C:\\Users\\Grace\\Documents\\Manga109-BoundingDete...   1654    1170   \n",
      "3      C:\\Users\\Grace\\Documents\\Manga109-BoundingDete...   1654    1170   \n",
      "4      C:\\Users\\Grace\\Documents\\Manga109-BoundingDete...   1654    1170   \n",
      "...                                                  ...    ...     ...   \n",
      "10125  C:\\Users\\Grace\\Documents\\Manga109-BoundingDete...   1654    1170   \n",
      "10126  C:\\Users\\Grace\\Documents\\Manga109-BoundingDete...   1654    1170   \n",
      "10127  C:\\Users\\Grace\\Documents\\Manga109-BoundingDete...   1654    1170   \n",
      "10128  C:\\Users\\Grace\\Documents\\Manga109-BoundingDete...   1654    1170   \n",
      "10129  C:\\Users\\Grace\\Documents\\Manga109-BoundingDete...   1654    1170   \n",
      "\n",
      "              book_id                                             bboxes  \\\n",
      "0                ARMS  [[83, 86, 668, 1004], [406, 684, 50, 80], [178...   \n",
      "1                ARMS  [[899, 585, 271, 500], [2, 0, 824, 513], [72, ...   \n",
      "2                ARMS  [[449, 593, 290, 499], [908, 6, 668, 466], [73...   \n",
      "3                ARMS  [[86, 91, 663, 337], [915, 568, 666, 531], [82...   \n",
      "4                ARMS  [[2, 672, 817, 425], [902, 94, 379, 218], [832...   \n",
      "...               ...                                                ...   \n",
      "10125  YumeiroCooking  [[597, 0, 149, 508], [910, 609, 221, 560], [91...   \n",
      "10126  YumeiroCooking  [[404, 258, 341, 468], [907, 520, 660, 188], [...   \n",
      "10127  YumeiroCooking  [[91, 70, 384, 237], [908, 317, 403, 156], [48...   \n",
      "10128  YumeiroCooking  [[1403, 480, 250, 647], [911, 70, 312, 262], [...   \n",
      "10129  YumeiroCooking  [[1345, 73, 225, 476], [1201, 76, 136, 286], [...   \n",
      "\n",
      "                                                  labels  \n",
      "0                                           [4, 1, 2, 3]  \n",
      "1      [4, 4, 4, 4, 4, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, ...  \n",
      "2      [4, 4, 4, 4, 4, 4, 4, 1, 1, 1, 1, 1, 1, 2, 2, ...  \n",
      "3      [4, 4, 4, 4, 4, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, ...  \n",
      "4      [4, 4, 4, 4, 4, 4, 4, 4, 1, 1, 1, 1, 1, 2, 2, ...  \n",
      "...                                                  ...  \n",
      "10125  [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 1, 1, 1, ...  \n",
      "10126  [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 1, ...  \n",
      "10127  [4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 1, 1, 1, 1, 1, ...  \n",
      "10128  [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, ...  \n",
      "10129  [4, 4, 4, 4, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
      "\n",
      "[10130 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# directory with Manga109 data \n",
    "manga109_root_dir = \"./Manga109/Manga109_released_2023_12_07\" # directory with Manga109 data\n",
    "\n",
    "# custom parser to access Manga109 data \n",
    "p = manga109api_custom.Parser(root_dir=manga109_root_dir)\n",
    "img_dict = p.load_all_images()\n",
    "\n",
    "# create pandas dataframe from the dictionary\n",
    "df = pd.DataFrame(img_dict)\n",
    "print(df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T21:23:43.632614Z",
     "start_time": "2024-04-07T21:23:34.749298Z"
    }
   },
   "id": "780b16e02f56126d",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "Split and Stratify Data "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb8ac8c465044628"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Images: 10130\n",
      "Training Images: 6483\n",
      "Validation Images: 1621\n",
      "Testing Images: 2026\n"
     ]
    }
   ],
   "source": [
    "test_ratio = 0.2\n",
    "val_ratio = 0.2\n",
    "\n",
    "# split data into training/validation and testing sets\n",
    "train_val_df, test_df = train_test_split(df, test_size=test_ratio, stratify=df['book_id'], random_state=42)\n",
    "\n",
    "# further split into training and validation sets \n",
    "train_df, val_df = train_test_split(train_val_df, test_size=val_ratio, stratify=train_val_df['book_id'], random_state=42)\n",
    "\n",
    "print(f\"Total Images: {len(df)}\")\n",
    "print(f\"Training Images: {len(train_df)}\")\n",
    "print(f\"Validation Images: {len(val_df)}\")\n",
    "print(f\"Testing Images: {len(test_df)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T21:43:33.841894Z",
     "start_time": "2024-04-07T21:43:33.813831Z"
    }
   },
   "id": "e176591c33fabaea",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "Transformations"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b364729dd91ae76"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# define training transformations\n",
    "train_transformations = A.Compose([\n",
    "    # resize? \n",
    "    #T.PILToTensor(),                            \n",
    "    #T.ConvertImageDtype(torch.float),           \n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    T.ColorJitter(brightness=(0.80, 1.20)),\n",
    "    A.Rotate(limit=90,p=0.5),\n",
    "    A.RandomCropFromBorders(crop_left=0.1, crop_right=0.1, crop_top=0.1, crop_bottom=0.1, always_apply=False, p=1.0)\n",
    "], bbox_params=A.BboxParams(format='coco'))\n",
    "\n",
    "# define validation transformations\n",
    "val_transformations = T.Compose([\n",
    "    # resize?\n",
    "    #T.PILToTensor(),\n",
    "    #T.ConvertImageDtype(torch.float)\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T18:39:29.984092Z",
     "start_time": "2024-04-07T18:39:29.976610Z"
    }
   },
   "id": "c335555ad0621fdb",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create Datasets & DataLoaders "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c33c7bd173bdece"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_images,train_transformations)\n",
    "val_dataset = CustomDataset(val_images)\n",
    "\n",
    "batch_size = 8\n",
    "num_workers = 4\n",
    "\n",
    "# image, target \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "valid_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T18:39:29.991151Z",
     "start_time": "2024-04-07T18:39:29.985094Z"
    }
   },
   "id": "abcb7430c9aaa0d7",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualize Data - testing & validation images  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a0a43f86d17652"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\Grace\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Grace\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"C:\\Users\\Grace\\Documents\\Manga109-BoundingDetection\\dataset.py\", line 59, in __getitem__\n    image, my_annotation = self.transform(image=image_np, bboxes=bboxes)\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Grace\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\albumentations\\core\\composition.py\", line 211, in __call__\n    p.ensure_data_valid(data)\n  File \"C:\\Users\\Grace\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\albumentations\\core\\bbox_utils.py\", line 116, in ensure_data_valid\n    raise ValueError(msg)\nValueError: Please specify 'label_fields' in 'bbox_params' or add labels to the end of bbox because bboxes must have labels\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdataset\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m visualize_image_with_boxes\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtransforms\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctional\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mF\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m image, annotation \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43miter\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    630\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 631\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    635\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1346\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1344\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1345\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_task_info[idx]\n\u001B[1;32m-> 1346\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_process_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1372\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._process_data\u001B[1;34m(self, data)\u001B[0m\n\u001B[0;32m   1370\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_try_put_index()\n\u001B[0;32m   1371\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, ExceptionWrapper):\n\u001B[1;32m-> 1372\u001B[0m     \u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreraise\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1373\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_utils.py:722\u001B[0m, in \u001B[0;36mExceptionWrapper.reraise\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    718\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m    719\u001B[0m     \u001B[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001B[39;00m\n\u001B[0;32m    720\u001B[0m     \u001B[38;5;66;03m# instantiate since we don't know how to\u001B[39;00m\n\u001B[0;32m    721\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 722\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exception\n",
      "\u001B[1;31mValueError\u001B[0m: Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\Grace\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Grace\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"C:\\Users\\Grace\\Documents\\Manga109-BoundingDetection\\dataset.py\", line 59, in __getitem__\n    image, my_annotation = self.transform(image=image_np, bboxes=bboxes)\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Grace\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\albumentations\\core\\composition.py\", line 211, in __call__\n    p.ensure_data_valid(data)\n  File \"C:\\Users\\Grace\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\albumentations\\core\\bbox_utils.py\", line 116, in ensure_data_valid\n    raise ValueError(msg)\nValueError: Please specify 'label_fields' in 'bbox_params' or add labels to the end of bbox because bboxes must have labels\n"
     ]
    }
   ],
   "source": [
    "from dataset import visualize_image_with_boxes\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "image, annotation = next(iter(train_loader))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T18:39:44.136593Z",
     "start_time": "2024-04-07T18:39:29.993156Z"
    }
   },
   "id": "a2176c866932c4f0",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define and Train Model "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c56b47c0ea845f9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = models.resnet34\n",
    "# optimizer = something \n",
    "# loss function = something \n",
    "# epochs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-07T18:39:44.137594Z"
    }
   },
   "id": "551aaea05c6e2845"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Validation Metrics \n",
    "- Testing Images Visualization \n",
    "- Testing \n",
    "- Testing Evaluation Metrics "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f76ffc506bbd1347"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
